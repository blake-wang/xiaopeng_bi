# 上线配置
## 数据库配置
#old: 10.45.191.46(moose) 10.116.123.113(rptdb)  10.24.167.208（hippo）
#new:172.17.2.79（moose_vpc） 172.17.2.81（rptdb_vpc） 172.19.8.47（hippo_vpc）
spark.local=false
jdbc.url=jdbc:mysql://172.17.2.79:3306/xiaopeng2_bi?user=hadoop&password=hadoopmaster2016abt&useUnicode=true&characterEncoding=utf-8&autoReconnect=true&failOverReadOnly=false
jdbc.user=hadoop
jdbc.pwd=hadoopmaster2016abt
jdbc.xiaopeng2.url=jdbc:mysql://172.17.2.81:3306/xiaopeng2?user=hadoop&password=hadoopmaster2016abt&useUnicode=true&characterEncoding=utf-8&autoReconnect=true&failOverReadOnly=false
jdbc.xiaopeng2.user=hadoop
jdbc.xiaopeng2.pwd=hadoopmaster2016abt
jdbc.xiaopeng2bihip.url=jdbc:mysql://172.19.8.47:3306/xiaopeng2_bi?user=hadoop&password=hadoopmaster2016abt&useUnicode=true&characterEncoding=utf-8&autoReconnect=true&failOverReadOnly=false
jdbc.xiaopeng2fx.url=jdbc:mysql://172.17.2.81:3306/xiaopeng2_faxing?user=hadoop&password=hadoopmaster2016abt&useUnicode=true&characterEncoding=utf-8&autoReconnect=true&failOverReadOnly=false&zeroDateTimeBehavior=convertToNull
jdbc.xiaopeng2fx.user=hadoop
jdbc.xiaopeng2fx.pwd=hadoopmaster2016abt
jdbc.driver=com.mysql.jdbc.Driver


# kafka 配置
kafka.metadata.broker.list=hadoopmaster:9092,hadoopslave1:9092,hadoopslave2:9092

kafka.topics.centurioncardaccount=login,regi,order
spark.checkpoint.centurioncardaccount=/home/hduser/spark/spark-1.6.1/checkpointdir/centurioncardaccounts

kafka.topics.apppoints=order,points
spark.checkpoint.apppoints=/home/hduser/spark/spark-1.6.1/checkpointdir/apppoints

kafka.topics.kpi=regi,order,login,active,pubgame,channel,request,thirddata
spark.checkpoint.kpi=file:///home/hduser/spark/spark-1.6.1/checkpointdir/checkpointkpi

kafka.topics.backend=member,regi,order,binduid,login,gameinner,accountonline,points,appdownload,appinstall
spark.checkpoint.backend=/home/hduser/spark/spark-1.6.1/checkpointdir/backend


#kafka.topics.member=member,regi,order,binduid
#spark.checkpoint.member=/home/hduser/spark/spark-1.6.1/checkpointdir/member


topicsApp=order,appdownload,login,appinstall
App2_2=file:///home/hduser/spark/spark-1.6.1/checkpointdir/App2_2

kafka.topics.alliance.kpi=regi,order,login,active
spark.checkpoint.alliance.kpi=file:///home/hduser/spark/spark-1.6.1/checkpointdir/checkpointalliancekpi

#离线任务 数据目录配置
gamepublish.offline.regi=hdfs://hadoopmaster:9000/user/hive/warehouse/yyft.db/regi/*
gamepublish.offline.order=hdfs://hadoopmaster:9000/user/hive/warehouse/yyft.db/order/*
gamepublish.offline.thirddata=hdfs://hadoopmaster:9000/user/hive/warehouse/yyft.db/thirddata/*

fxdim.parquet=hdfs://hadoopmaster:9000/tmp/hive/fxdim.parquet

# spark 参数配置
coalesce.partitioin.num=40
spark.sql.shuffle.partitions=40
spark.memory.storageFraction=0.2

#redis相关配置 #old:10.46.133.54 #new vpc:172.19.8.48  先迁移数据库时 还是用老环境的redis
redis.host=172.19.8.48
redis.port=6379
redis.max.idle=50
redis.max.total=1000
redis.max.wait.millis=10000

#web交互的模式
web.url.mode=product

#kafka regi日志缓存一批目录地址
regi_log_cache=hdfs://hadoopmaster:9000/home/hduser/spark/spark-1.6.1/checkpointdir/thirddata_regi_log_cache
